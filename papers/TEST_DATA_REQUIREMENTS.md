# TODO: Required Test Data for ModelCypher Papers

> **CRITICAL**: This document specifies **real experimental data** that must be generated by running actual models. **No simulated, placeholder, or synthetic data is acceptable.**

---

## Paper 1: Manifold Hypothesis of Agency

### TODO: Semantic Prime Embeddings

**What**: Extract token embedding vectors for 65 NSM semantic primes from each model.

**How**:
```bash
# Use the existing geometry CLI
mc geometry primes probe --model mlx-community/Qwen2.5-3B-Instruct-4bit --output data/paper1/qwen2.5-3b.json
mc geometry primes probe --model mlx-community/Llama-3.2-3B-Instruct-4bit --output data/paper1/llama-3.2-3b.json
```

**Required models**:
- [ ] Qwen2.5-0.5B-Instruct
- [ ] Qwen2.5-1.5B-Instruct  
- [ ] Qwen2.5-3B-Instruct
- [ ] Llama-3.2-1B-Instruct
- [ ] Llama-3.2-3B-Instruct
- [ ] TinyLlama-1.1B-Chat

**Output format**: JSON with `{prime: string, embedding: float[]}` for each prime.

---

### TODO: Gram Matrix Comparisons

**What**: Compute Gram matrices (G = X X^T after mean-centering) for prime embeddings.

**How**: 
```python
from modelcypher.core.domain.geometry.concept_response_matrix import ConceptResponseMatrix
# Use ConceptResponseMatrix to compute and compare Gram matrices
```

**Deliverables**:
- [ ] `data/paper1/gram_matrices/` â€” NumPy files per model
- [ ] `data/paper1/cka_pairwise.csv` â€” CKA scores for all model pairs

---

### TODO: Null Distribution for Statistical Significance

**What**: Compare prime CKA against 200 random word subsets of same size.

**Required data**:
- [ ] Frequency-matched control word list (n=200)
- [ ] 200 random subset CKA measurements
- [ ] p-value computation for prime CKA vs null

---

## Paper 2: Linguistic Thermodynamics

### TODO: Entropy Under Intensity Modifiers

**What**: Measure token-level entropy for prompts with/without intensity modifiers.

**Required experiments**:
- [ ] 20 refusal-prone prompts (e.g., "How do I pick a lock?")
- [ ] 20 neutral prompts (e.g., "How do I cook pasta?")
- [ ] 10 modifiers (caps, urgency, roleplay, negation, etc.)
- [ ] 4 models (Qwen, Llama, Mistral, TinyLlama)
- [ ] Temperatures: 0.0, 0.3, 0.7, 1.0

**How**:
```bash
mc entropy measure --model <id> --prompt "<prompt>" --modifier "<modifier>" --temperature <T>
```

**Deliverables**:
- [ ] `data/paper2/modifier_entropy.csv` â€” Full 20Ã—10Ã—4 matrix with mean entropy
- [ ] `data/paper2/temperature_sweep.csv` â€” Entropy vs temperature curves

---

### TODO: Safety Signal AUROC Comparison

**What**: Compare entropy vs Î”H vs KL for harmful/benign prompt classification.

**Required**:
- [ ] Curated harmful prompt set (100 prompts) â€” **human review required**
- [ ] Curated benign prompt set (100 prompts, matched topics)
- [ ] AUROC computation for each signal type

**Deliverables**:
- [ ] `data/paper2/safety_auroc.csv` â€” Signal Ã— Model AUROC matrix

---

## Paper 3: Unified Manifold Alignment

### TODO: Intersection Maps

**What**: Layer-wise overlap analysis between model pairs.

**Required model pairs**:
- [ ] Qwen2.5-3B â†” Llama-3.2-3B
- [ ] Qwen2.5-7B â†” Mistral-7B

**How**:
```bash
mc geometry stitch analyze --source <model_a> --target <model_b> --output data/paper3/intersection/
```

**Deliverables**:
- [ ] `data/paper3/intersection_maps/` â€” JSON per pair
- [ ] Layer coverage scores
- [ ] Jaccard overlap per layer

---

### TODO: Merge Evaluation Suites

**What**: Measure skill retention after cross-family adapter transfer.

**Required suites**:
- [ ] Coding: 50 problems from HumanEval or similar
- [ ] Creative: 50 prompts with human-graded rubrics

**Baselines**:
- [ ] Source model (pre-transfer)
- [ ] Target model (no adapter)
- [ ] Naive weight average
- [ ] TIES-Merging
- [ ] Our method (anchor-locked Procrustes)

---

## Paper 4: ModelCypher Toolkit

### TODO: Benchmark Comparisons

**What**: Feature comparison against related tools.

| Tool | Comparison Point |
|------|-----------------|
| TransformerLens | Module count, feature coverage |
| CircuitsVis | Visualization capabilities |
| mergekit | Merge method support |
| LM-Eval | Eval integration |

**Deliverables**:
- [ ] Feature comparison table (manual audit)
- [ ] Runtime benchmarks (optional)

---

## Execution Priority

| Priority | Task | Blocking Papers |
|----------|------|-----------------|
| ðŸ”´ P0 | Semantic prime embeddings | Paper 1, Paper 4 |
| ðŸ”´ P0 | Modifier entropy matrix | Paper 2, Paper 4 |
| ðŸŸ¡ P1 | Null distribution | Paper 1 |
| ðŸŸ¡ P1 | Safety AUROC | Paper 2 |
| ðŸŸ¢ P2 | Intersection maps | Paper 3 |
| ðŸŸ¢ P2 | Merge eval suites | Paper 3 |

---

## Data Integrity Requirements

1. **No simulated data** â€” All values must come from actual model inference
2. **Reproducibility** â€” All experiments must be runnable from CLI commands
3. **Version pinning** â€” Record exact model IDs and commit hashes
4. **Human review** â€” Safety-related prompts require manual curation
