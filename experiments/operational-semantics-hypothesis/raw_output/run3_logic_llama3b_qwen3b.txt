{"level": "info", "message": "Extracting fingerprints from source model...", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Loading model from /Volumes/CodeCypher/models/mlx-community/Llama-3.2-3B-Instruct-4bit for fingerprinting (9 probes)", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Model loaded: 28 layers", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Extracted 9 fingerprints from 9 probes", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Cached fingerprints for Llama-3.2-3B-Instruct-4bit (9 probes)", "logger": "modelcypher.core.domain.geometry.fingerprint_cache"}
{"level": "info", "message": "Extracting fingerprints from target model...", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Loading model from /Volumes/CodeCypher/models/mlx-community/Qwen2.5-3B-Instruct-bf16 for fingerprinting (9 probes)", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Model loaded: 36 layers", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Extracted 9 fingerprints from 9 probes", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Cached fingerprints for Qwen2.5-3B-Instruct-bf16 (9 probes)", "logger": "modelcypher.core.domain.geometry.fingerprint_cache"}
{"_schema": "mc.geometry.invariant.map_layers.v1", "alignmentQuality": 0.6646937658956544, "atlasDomainsDetected": 0, "atlasSourcesDetected": 0, "interpretation": "Layer mapping quality: good (alignment 0.665) | Mapped 12 layers, skipped 0 | Triangulation quality: high (multiplier 1.50)", "invariantCount": 9, "invariantScope": "sequenceInvariants", "mappedLayers": 12, "mappings": [{"confidence": "medium", "isSkipped": false, "similarity": 0.4702809942740426, "sourceLayer": 0, "targetLayer": 0}, {"confidence": "medium", "isSkipped": false, "similarity": 0.48632465568298455, "sourceLayer": 2, "targetLayer": 3}, {"confidence": "medium", "isSkipped": false, "similarity": 0.5384175177408745, "sourceLayer": 5, "targetLayer": 6}, {"confidence": "medium", "isSkipped": false, "similarity": 0.5626479775089176, "sourceLayer": 7, "targetLayer": 10}, {"confidence": "medium", "isSkipped": false, "similarity": 0.585875434524414, "sourceLayer": 10, "targetLayer": 13}, {"confidence": "medium", "isSkipped": false, "similarity": 0.6065207925970513, "sourceLayer": 12, "targetLayer": 16}, {"confidence": "medium", "isSkipped": false, "similarity": 0.6430672415557716, "sourceLayer": 15, "targetLayer": 19}, {"confidence": "high", "isSkipped": false, "similarity": 0.6584035855335972, "sourceLayer": 17, "targetLayer": 22}, {"confidence": "high", "isSkipped": false, "similarity": 0.7152265490825167, "sourceLayer": 20, "targetLayer": 25}, {"confidence": "high", "isSkipped": false, "similarity": 0.7776273417082008, "sourceLayer": 22, "targetLayer": 29}, {"confidence": "high", "isSkipped": false, "similarity": 0.9319331005394825, "sourceLayer": 25, "targetLayer": 32}, {"confidence": "high", "isSkipped": false, "similarity": 1.0, "sourceLayer": 27, "targetLayer": 35}], "meanSimilarity": 0.6646937658956544, "meanTriangulationMultiplier": 1.5, "recommendedAction": "Layer mapping complete. Review correspondence before merge.", "skippedLayers": 0, "sourceCollapsedLayers": 0, "sourceModel": "/Volumes/CodeCypher/models/mlx-community/Llama-3.2-3B-Instruct-4bit", "targetCollapsedLayers": 0, "targetModel": "/Volumes/CodeCypher/models/mlx-community/Qwen2.5-3B-Instruct-bf16", "totalProbesUsed": 9, "triangulationQuality": "high"}
