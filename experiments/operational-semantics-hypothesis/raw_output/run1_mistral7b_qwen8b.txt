{"level": "info", "message": "Extracting fingerprints from source model...", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Loading model from /Volumes/CodeCypher/models/mlx-community/Mistral-7B-Instruct-v0.3-4bit for fingerprinting (343 probes)", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Model loaded: 32 layers", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Extracted 343 fingerprints from 343 probes", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Cached fingerprints for Mistral-7B-Instruct-v0.3-4bit (343 probes)", "logger": "modelcypher.core.domain.geometry.fingerprint_cache"}
{"level": "info", "message": "Extracting fingerprints from target model...", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Loading model from /Volumes/CodeCypher/models/mlx-community/Qwen3-8B-4bit for fingerprinting (343 probes)", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Model loaded: 36 layers", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Extracted 343 fingerprints from 343 probes", "logger": "modelcypher.core.use_cases.invariant_layer_mapping_service"}
{"level": "info", "message": "Cached fingerprints for Qwen3-8B-4bit (343 probes)", "logger": "modelcypher.core.domain.geometry.fingerprint_cache"}
{"_schema": "mc.geometry.invariant.map_layers.v1", "alignmentQuality": 0.568171937182733, "atlasDomainsDetected": 11, "atlasSourcesDetected": 8, "interpretation": "Layer mapping quality: good (alignment 0.568) | Mapped 12 layers, skipped 3 | Triangulation quality: high (multiplier 2.06) | Multi-atlas: 8 sources, 11 domains, 343 probes", "invariantCount": 343, "invariantScope": "multiAtlas", "mappedLayers": 12, "mappings": [{"confidence": "medium", "isSkipped": false, "similarity": 0.47799375589726484, "sourceLayer": 0, "targetLayer": 0}, {"confidence": "medium", "isSkipped": false, "similarity": 0.48127022254159907, "sourceLayer": 3, "targetLayer": 3}, {"confidence": "uncertain", "isSkipped": true, "similarity": 0.09652388316942298, "sourceLayer": 6, "targetLayer": 6}, {"confidence": "uncertain", "isSkipped": true, "similarity": 0.1309252386541774, "sourceLayer": 8, "targetLayer": 10}, {"confidence": "uncertain", "isSkipped": true, "similarity": 0.14524480076011403, "sourceLayer": 11, "targetLayer": 13}, {"confidence": "low", "isSkipped": false, "similarity": 0.30405325470737093, "sourceLayer": 14, "targetLayer": 16}, {"confidence": "low", "isSkipped": false, "similarity": 0.33733942460996014, "sourceLayer": 17, "targetLayer": 19}, {"confidence": "low", "isSkipped": false, "similarity": 0.4151511146876117, "sourceLayer": 20, "targetLayer": 22}, {"confidence": "medium", "isSkipped": false, "similarity": 0.5453351785820342, "sourceLayer": 23, "targetLayer": 25}, {"confidence": "high", "isSkipped": false, "similarity": 0.7031663088118653, "sourceLayer": 25, "targetLayer": 29}, {"confidence": "high", "isSkipped": false, "similarity": 0.8492381748068908, "sourceLayer": 28, "targetLayer": 32}, {"confidence": "high", "isSkipped": false, "similarity": 1.0, "sourceLayer": 31, "targetLayer": 35}], "meanSimilarity": 0.45718677976902594, "meanTriangulationMultiplier": 2.061552812808829, "recommendedAction": "Layer mapping complete. Review correspondence before merge.", "skippedLayers": 3, "sourceCollapsedLayers": 0, "sourceModel": "/Volumes/CodeCypher/models/mlx-community/Mistral-7B-Instruct-v0.3-4bit", "targetCollapsedLayers": 0, "targetModel": "/Volumes/CodeCypher/models/mlx-community/Qwen3-8B-4bit", "totalProbesUsed": 343, "triangulationQuality": "high"}
